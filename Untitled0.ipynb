{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adminmodelex/website-modelex/blob/gh-pages/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **English-to-Spanish translation**\n",
        "**Sequence-to-sequence Transformer model on the machine translation task**"
      ],
      "metadata": {
        "id": "q6QOIsf6TM9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Simon-Pierre Boucher*"
      ],
      "metadata": {
        "id": "EMdyM7pQnTQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge-score\n",
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRN69kRtTKdc",
        "outputId": "6f798735-3dc4-46a5-8ecd-13844f7112f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import pathlib\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9VkmdGVTkez",
        "outputId": "aa768367-ddaa-4658-f1b1-0a9c9cb8264e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "SPA_VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ],
      "metadata": {
        "id": "7xRIp-cdTl65"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "Vhvgg2lGTorX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    eng = eng.lower()\n",
        "    spa = spa.lower()\n",
        "    text_pairs.append((eng, spa))\n"
      ],
      "metadata": {
        "id": "g0U_nY8GT061"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTPl1YhKT4M-",
        "outputId": "62e56c9b-53db-4924-abb3-3770f613140d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('no one will believe me.', 'nadie me va a creer.')\n",
            "(\"i'm sure tom will agree to help.\", 'estoy seguro de que tom querrá ayudarte.')\n",
            "(\"we expect that he'll help us.\", 'esperamos que nos ayude.')\n",
            "('i have a bad pain in my back.', 'tengo un fuerte dolor en la espalda.')\n",
            "('did you receive my letter?', '¿recibiste mi carta?')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QfWFda_T6dU",
        "outputId": "cef087cf-03e6-460d-a6eb-4e5959ca7823"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "FuzGDwAjT8y9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
      ],
      "metadata": {
        "id": "naXc51w4T-5s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English Tokens: \", eng_vocab[100:110])\n",
        "print(\"Spanish Tokens: \", spa_vocab[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUdq3SnlT_E7",
        "outputId": "f281c9b6-ac7d-428b-ea82-2435e1f5d9ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens:  ['like', 'at', 'know', 'him', 'there', 'they', 'go', 'her', 'has', 'will']\n",
            "Spanish Tokens:  ['está', 'con', 'le', 'ella', 'qué', 'te', 'para', 'mary', 'las', 'más']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab, lowercase=False\n",
        ")\n",
        "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=spa_vocab, lowercase=False\n",
        ")"
      ],
      "metadata": {
        "id": "vJdazKWyT_MN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_input_ex = text_pairs[0][0]\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(\"English sentence: \", eng_input_ex)\n",
        "print(\"Tokens: \", eng_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "spa_input_ex = text_pairs[0][1]\n",
        "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
        "print(\"Spanish sentence: \", spa_input_ex)\n",
        "print(\"Tokens: \", spa_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjDT_p9fT_PB",
        "outputId": "5f14216f-291b-4492-c3e6-91f2ebf53a17"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:  i really don't want to go to boston with tom.\n",
            "Tokens:  tf.Tensor([ 35 177  87   8  46  96  68 106  68 228  94  70  12], shape=(13,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b\"i really don ' t want to go to boston with tom .\", shape=(), dtype=string)\n",
            "\n",
            "Spanish sentence:  sinceramente, yo no quiero ir a boston con tom.\n",
            "Tokens:  tf.Tensor([3617   13  111   83  121  143   30  224  101   84   15], shape=(11,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b'sinceramente , yo no quiero ir a boston con tom .', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(eng, spa):\n",
        "    batch_size = tf.shape(spa)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    spa = spa_tokenizer(spa)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n",
        "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "vOGRs52aUH-5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdx4O83sUIFt",
        "outputId": "62ac7f42-2305-43a0-9361-465c33fc96f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "7KVv6YaGUII1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2pdsBVRUILV",
        "outputId": "15b66fe9-d125-45ac-ee4f-12b64d551d39"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " token_and_position_embeddi  (None, None, 256)            3850240   ['encoder_inputs[0][0]']      \n",
            " ng (TokenAndPositionEmbedd                                                                       \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
            " formerEncoder)                                                     [0][0]']                      \n",
            "                                                                                                  \n",
            " model_1 (Functional)        (None, None, 15000)          9283992   ['decoder_inputs[0][0]',      \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14449304 (55.12 MB)\n",
            "Trainable params: 14449304 (55.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "1302/1302 [==============================] - 79s 51ms/step - loss: 3.5504 - accuracy: 0.4662 - val_loss: 2.6103 - val_accuracy: 0.5704\n",
            "Epoch 2/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 2.5089 - accuracy: 0.5885 - val_loss: 2.2028 - val_accuracy: 0.6250\n",
            "Epoch 3/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 2.1957 - accuracy: 0.6307 - val_loss: 2.0441 - val_accuracy: 0.6492\n",
            "Epoch 4/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 2.0266 - accuracy: 0.6566 - val_loss: 2.0002 - val_accuracy: 0.6570\n",
            "Epoch 5/20\n",
            "1302/1302 [==============================] - 51s 39ms/step - loss: 1.9156 - accuracy: 0.6745 - val_loss: 1.9507 - val_accuracy: 0.6704\n",
            "Epoch 6/20\n",
            "1302/1302 [==============================] - 50s 38ms/step - loss: 1.8349 - accuracy: 0.6878 - val_loss: 1.9373 - val_accuracy: 0.6773\n",
            "Epoch 7/20\n",
            "1302/1302 [==============================] - 50s 38ms/step - loss: 1.7749 - accuracy: 0.6984 - val_loss: 1.9362 - val_accuracy: 0.6789\n",
            "Epoch 8/20\n",
            "1302/1302 [==============================] - 50s 38ms/step - loss: 1.7220 - accuracy: 0.7077 - val_loss: 1.9446 - val_accuracy: 0.6819\n",
            "Epoch 9/20\n",
            "1302/1302 [==============================] - 51s 39ms/step - loss: 1.6799 - accuracy: 0.7149 - val_loss: 1.9541 - val_accuracy: 0.6818\n",
            "Epoch 10/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.6393 - accuracy: 0.7213 - val_loss: 1.9623 - val_accuracy: 0.6842\n",
            "Epoch 11/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.6052 - accuracy: 0.7280 - val_loss: 1.9822 - val_accuracy: 0.6818\n",
            "Epoch 12/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.5740 - accuracy: 0.7331 - val_loss: 1.9770 - val_accuracy: 0.6878\n",
            "Epoch 13/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.5425 - accuracy: 0.7385 - val_loss: 2.0103 - val_accuracy: 0.6847\n",
            "Epoch 14/20\n",
            "1302/1302 [==============================] - 51s 39ms/step - loss: 1.5124 - accuracy: 0.7435 - val_loss: 1.9961 - val_accuracy: 0.6886\n",
            "Epoch 15/20\n",
            "1302/1302 [==============================] - 50s 38ms/step - loss: 1.4852 - accuracy: 0.7480 - val_loss: 2.0109 - val_accuracy: 0.6884\n",
            "Epoch 16/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.4615 - accuracy: 0.7519 - val_loss: 2.0689 - val_accuracy: 0.6812\n",
            "Epoch 17/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.4375 - accuracy: 0.7560 - val_loss: 2.0322 - val_accuracy: 0.6902\n",
            "Epoch 18/20\n",
            "1302/1302 [==============================] - 51s 39ms/step - loss: 1.4159 - accuracy: 0.7600 - val_loss: 2.0439 - val_accuracy: 0.6882\n",
            "Epoch 19/20\n",
            "1302/1302 [==============================] - 49s 38ms/step - loss: 1.3970 - accuracy: 0.7634 - val_loss: 2.0666 - val_accuracy: 0.6894\n",
            "Epoch 20/20\n",
            "1302/1302 [==============================] - 48s 37ms/step - loss: 1.3728 - accuracy: 0.7674 - val_loss: 2.0868 - val_accuracy: 0.6897\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfb4c58e230>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = tf.shape(input_sentences)[0]\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(\n",
        "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
        "    )\n",
        "\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = tf.fill((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = tf.fill((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = tf.concat((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        end_token_id=spa_tokenizer.token_to_id(\"[END]\"),\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(2):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences(tf.constant([input_sentence]))\n",
        "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8uii1IjUIUe",
        "outputId": "2b59988a-4036-400b-c212-2dd6ee02d58b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "i don't remember anything about them.\n",
            "no me acuerdo de nada de ellos .\n",
            "\n",
            "** Example 1 **\n",
            "tom doesn't know who he should ask.\n",
            "tom no sabe quién debería preguntar .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
        "\n",
        "for test_pair in test_pairs[:30]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequences(tf.constant([input_sentence]))\n",
        "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
        "    translated_sentence = (\n",
        "        translated_sentence.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    rouge_1(reference_sentence, translated_sentence)\n",
        "    rouge_2(reference_sentence, translated_sentence)\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())"
      ],
      "metadata": {
        "id": "FD15_q7ZUd0u",
        "outputId": "982618fa-1ce0-4d47-ee5e-1faac94d5aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.5863939>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.58538>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.57980007>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.36652622>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.36894113>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.3627912>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = tf.shape(input_sentences)[0]\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(\n",
        "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
        "    )\n",
        "\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = tf.fill((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = tf.fill((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = tf.concat((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        end_token_id=spa_tokenizer.token_to_id(\"[END]\"),\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(200):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences(tf.constant([input_sentence]))\n",
        "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ],
      "metadata": {
        "id": "OCeSSN1qLG2A",
        "outputId": "bdc0cde6-7ce0-49b0-b7b4-d046bf1bd42a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "tom never loved you.\n",
            "tom nunca te quería .\n",
            "\n",
            "** Example 1 **\n",
            "do you have a soup bowl?\n",
            "¿ tienes un lado del diar ?\n",
            "\n",
            "** Example 2 **\n",
            "i have a cousin who is a lawyer.\n",
            "tengo una primo que es abogado .\n",
            "\n",
            "** Example 3 **\n",
            "come quick!\n",
            "¡ vamos !\n",
            "\n",
            "** Example 4 **\n",
            "the situation gets worse and worse.\n",
            "la situación empeora y peor .\n",
            "\n",
            "** Example 5 **\n",
            "there's something else.\n",
            "hay algo más .\n",
            "\n",
            "** Example 6 **\n",
            "grab a broom and help us clean.\n",
            "apúda un escoba su ayuda para nosotros .\n",
            "\n",
            "** Example 7 **\n",
            "there are no problems.\n",
            "no hay ningún problema .\n",
            "\n",
            "** Example 8 **\n",
            "how did you learn to speak french so well?\n",
            "¿ cómo te aprender tan bien francés ?\n",
            "\n",
            "** Example 9 **\n",
            "tom hates olives.\n",
            "tom odia ocases .\n",
            "\n",
            "** Example 10 **\n",
            "don't believe him.\n",
            "no le te crees .\n",
            "\n",
            "** Example 11 **\n",
            "tom hopes that it doesn't snow tomorrow.\n",
            "tom espera que no nieve mañana .\n",
            "\n",
            "** Example 12 **\n",
            "i love my dad.\n",
            "me encanta mi papá .\n",
            "\n",
            "** Example 13 **\n",
            "this is japan.\n",
            "esto es japón .\n",
            "\n",
            "** Example 14 **\n",
            "don't play dumb. i know you.\n",
            "no jues , no te conozco .\n",
            "\n",
            "** Example 15 **\n",
            "that's my umbrella.\n",
            "ese es mi paraguas .\n",
            "\n",
            "** Example 16 **\n",
            "tom was caught lying.\n",
            "tom estaba reperado .\n",
            "\n",
            "** Example 17 **\n",
            "how much is the fine for speeding?\n",
            "¿ cuánto tiempo tiene la velocidad por velocidad ?\n",
            "\n",
            "** Example 18 **\n",
            "we'll do it.\n",
            "lo haremos .\n",
            "\n",
            "** Example 19 **\n",
            "tom gave mary his telephone number.\n",
            "tom le dio su número de teléfono a mary .\n",
            "\n",
            "** Example 20 **\n",
            "tom was a judge in an art contest.\n",
            "tom era un juez en un concurso de arte .\n",
            "\n",
            "** Example 21 **\n",
            "the island was enveloped in a thick fog.\n",
            "la isla fue con en una nieblaampeagua .\n",
            "\n",
            "** Example 22 **\n",
            "tom is growing up.\n",
            "tom está anturrió .\n",
            "\n",
            "** Example 23 **\n",
            "tom has accumulated a lot of junk since he's moved into this house.\n",
            "tom ha acumpunta mucho desde que conce a la casa con modeceder este esta casa .\n",
            "\n",
            "** Example 24 **\n",
            "this isn't the first time this has happened.\n",
            "esto no es la primera vez que pasó .\n",
            "\n",
            "** Example 25 **\n",
            "you're the only person who shows me any respect.\n",
            "eres la única persona que me ve el único respeta .\n",
            "\n",
            "** Example 26 **\n",
            "i am very reasonable with my employees.\n",
            "soy muy razonable con mis empleados .\n",
            "\n",
            "** Example 27 **\n",
            "he didn't want to think about it.\n",
            "él no quería pensar en eso .\n",
            "\n",
            "** Example 28 **\n",
            "he twirled his mustache.\n",
            "él se arro le tiene dolor de mudar .\n",
            "\n",
            "** Example 29 **\n",
            "tom won the argument.\n",
            "tom ganó la discusión .\n",
            "\n",
            "** Example 30 **\n",
            "she looked all around.\n",
            "ella miró todos alrededor .\n",
            "\n",
            "** Example 31 **\n",
            "how did you decide?\n",
            "¿ cómo te descmpempieta ?\n",
            "\n",
            "** Example 32 **\n",
            "i put bait on the hook.\n",
            "he puesto un andilla .\n",
            "\n",
            "** Example 33 **\n",
            "i don't want to be intrusive, but is everything all right?\n",
            "no quiero estar en astrusias , pero sí ?\n",
            "\n",
            "** Example 34 **\n",
            "tom hung a calendar on the wall.\n",
            "tom le coltó una mura paredalda de la pared .\n",
            "\n",
            "** Example 35 **\n",
            "count to one hundred.\n",
            "cuenta a muchos cien años .\n",
            "\n",
            "** Example 36 **\n",
            "this work's very accurately done.\n",
            "este trabajo es muy incurrió mucho .\n",
            "\n",
            "** Example 37 **\n",
            "delegates debated the compromise for many days.\n",
            "los nepostó la compromenea de muchos días .\n",
            "\n",
            "** Example 38 **\n",
            "we heard the tree fall with a crash.\n",
            "oímos el árbol con un caulá en un accidente .\n",
            "\n",
            "** Example 39 **\n",
            "you're cuter than mary.\n",
            "eres más mocruena que maría .\n",
            "\n",
            "** Example 40 **\n",
            "let me take your blood pressure.\n",
            "déjame tomarle una mala mala mala mala .\n",
            "\n",
            "** Example 41 **\n",
            "i'm sure tom would rather be somewhere else.\n",
            "estoy seguro de que tom preferiría estar en algún otro lugar .\n",
            "\n",
            "** Example 42 **\n",
            "when was the last time you plucked a chicken?\n",
            "¿ cuándo fue el último tiempo que aportaste una pollo ?\n",
            "\n",
            "** Example 43 **\n",
            "it hurts.\n",
            "duele .\n",
            "\n",
            "** Example 44 **\n",
            "what do you want santa to bring you for christmas?\n",
            "¿ qué quieres a santar ?\n",
            "\n",
            "** Example 45 **\n",
            "put more salt in the soup.\n",
            "a la salvava la sopa .\n",
            "\n",
            "** Example 46 **\n",
            "i want to spend all my time with you.\n",
            "quiero pasar todo el tiempo con vos .\n",
            "\n",
            "** Example 47 **\n",
            "tom laughs at his own jokes.\n",
            "tom se rías de sus propios bromas .\n",
            "\n",
            "** Example 48 **\n",
            "my brother is holding a camera in his hand.\n",
            "mi hermano tiene una mano en su mano .\n",
            "\n",
            "** Example 49 **\n",
            "why don't you ever cook?\n",
            "¿ por qué no te cocinas ?\n",
            "\n",
            "** Example 50 **\n",
            "non-members pay an additional 50 dollars.\n",
            "los miembros no pagar una gradático de cincuenta dólares .\n",
            "\n",
            "** Example 51 **\n",
            "accuracy is important in arithmetic.\n",
            "la astigriva es importante en una fompez .\n",
            "\n",
            "** Example 52 **\n",
            "the streets are clean.\n",
            "las calles están limpios .\n",
            "\n",
            "** Example 53 **\n",
            "the rain prevented me from going.\n",
            "la lluvia me impidió ir .\n",
            "\n",
            "** Example 54 **\n",
            "i think she's hiding something.\n",
            "creo que ella está escondido otra .\n",
            "\n",
            "** Example 55 **\n",
            "he asked us not to make any noise.\n",
            "él nos pidió que no hagas ruido .\n",
            "\n",
            "** Example 56 **\n",
            "have you ever grown a beard?\n",
            "¿ has acor alguna vez la barba ?\n",
            "\n",
            "** Example 57 **\n",
            "i wanted to know why you didn't come yesterday.\n",
            "quería saber por qué no has venido ayer .\n",
            "\n",
            "** Example 58 **\n",
            "tom could hardly stand the pain.\n",
            "tom tan casi no podía soportar el dolor .\n",
            "\n",
            "** Example 59 **\n",
            "when do you expect him back?\n",
            "¿ cuándo te devuelvera ?\n",
            "\n",
            "** Example 60 **\n",
            "she was wearing a splendid outfit.\n",
            "ella llevaba puesto una es hiberi con mucha donma .\n",
            "\n",
            "** Example 61 **\n",
            "tom said he had a plan.\n",
            "tom dijo que tenía un plan .\n",
            "\n",
            "** Example 62 **\n",
            "i'm out of breath.\n",
            "estoy sin respiración .\n",
            "\n",
            "** Example 63 **\n",
            "tom would've helped us if we'd asked him.\n",
            "tom nos habría ayudó ay si le pediste que le peba .\n",
            "\n",
            "** Example 64 **\n",
            "i can't believe i'm here.\n",
            "no puedo creer que estoy aquí .\n",
            "\n",
            "** Example 65 **\n",
            "everyone hates me.\n",
            "todos me odian .\n",
            "\n",
            "** Example 66 **\n",
            "i hurried in order not to be late for school.\n",
            "me apresura orden no ser la escuela por primera escuela .\n",
            "\n",
            "** Example 67 **\n",
            "he is her friend.\n",
            "él es su amiga .\n",
            "\n",
            "** Example 68 **\n",
            "she is no less beautiful than her mother.\n",
            "ella no es tan hermosa que la madre le madre .\n",
            "\n",
            "** Example 69 **\n",
            "i want to live in a small town.\n",
            "quiero vivir en una ciudad pequeña .\n",
            "\n",
            "** Example 70 **\n",
            "i can't afford a car.\n",
            "no puedo dejar coche .\n",
            "\n",
            "** Example 71 **\n",
            "the king once lived in that palace.\n",
            "el rey vivió una vez en ese paltiva .\n",
            "\n",
            "** Example 72 **\n",
            "it's a pity they're getting divorced.\n",
            "es una pena divorciarse .\n",
            "\n",
            "** Example 73 **\n",
            "i hope it won't rain this afternoon.\n",
            "espero que no llueva esta tarde .\n",
            "\n",
            "** Example 74 **\n",
            "look out for bees.\n",
            "miras por las atento .\n",
            "\n",
            "** Example 75 **\n",
            "the first step is the most difficult.\n",
            "el primero es prooro es el diruncer más difícil .\n",
            "\n",
            "** Example 76 **\n",
            "he has no remorse.\n",
            "él no tiene rembeemo .\n",
            "\n",
            "** Example 77 **\n",
            "i'm impatient.\n",
            "estoy muys ¡ maran !\n",
            "\n",
            "** Example 78 **\n",
            "three of my children died.\n",
            "tres de mis hijos murieron .\n",
            "\n",
            "** Example 79 **\n",
            "how many strokes does the kanji for \"michi\" have?\n",
            "¿ cuántos caletri el khanfali el micán . \"\n",
            "\n",
            "** Example 80 **\n",
            "since i got married, i've quit working.\n",
            "como yo estaba casada , he ido de haber ido .\n",
            "\n",
            "** Example 81 **\n",
            "here comes tom.\n",
            "aquí viene tom .\n",
            "\n",
            "** Example 82 **\n",
            "tom wrote mary a love letter.\n",
            "tom le escribió una carta de amor .\n",
            "\n",
            "** Example 83 **\n",
            "what tom really wanted was a two-week vacation.\n",
            "lo que tom era realmente quería una semana diversa .\n",
            "\n",
            "** Example 84 **\n",
            "i have to thank him.\n",
            "tengo que agradecerlo .\n",
            "\n",
            "** Example 85 **\n",
            "i like my house.\n",
            "me gusta mi casa .\n",
            "\n",
            "** Example 86 **\n",
            "tom can't swim and neither can mary.\n",
            "tom no puede nadar y mary ni a ninguno .\n",
            "\n",
            "** Example 87 **\n",
            "look how that ball bounces.\n",
            "mira cómo se las gouleato .\n",
            "\n",
            "** Example 88 **\n",
            "sometimes i have the impression that we'll never come to an agreement.\n",
            "a veces tengo la impresión de que nunca vamos a un acuerdo .\n",
            "\n",
            "** Example 89 **\n",
            "give me a day.\n",
            "dame un día .\n",
            "\n",
            "** Example 90 **\n",
            "today is saturday.\n",
            "hoy es sábado .\n",
            "\n",
            "** Example 91 **\n",
            "tom lives on the other side of boston.\n",
            "tom vive al otro lado de boston .\n",
            "\n",
            "** Example 92 **\n",
            "it is easy to play tennis.\n",
            "es fácil jugar tenis .\n",
            "\n",
            "** Example 93 **\n",
            "please tell us why you need to stay here.\n",
            "por favor , dime por qué te tienes que quedarme aquí .\n",
            "\n",
            "** Example 94 **\n",
            "i don't want any more trouble.\n",
            "no quiero más problemas .\n",
            "\n",
            "** Example 95 **\n",
            "stephen king writes about evil people.\n",
            "prohiíbeta al rey a las personas .\n",
            "\n",
            "** Example 96 **\n",
            "the dog is bleeding.\n",
            "el perro esmordinaria .\n",
            "\n",
            "** Example 97 **\n",
            "tom is a teaching assistant.\n",
            "tom es una asustaraz .\n",
            "\n",
            "** Example 98 **\n",
            "she's just a child.\n",
            "ella es solo una niña .\n",
            "\n",
            "** Example 99 **\n",
            "she made me a cake.\n",
            "ella me hizo una tarta .\n",
            "\n",
            "** Example 100 **\n",
            "have you ever read any chinese poems?\n",
            "¿ has leído alguna vez ?\n",
            "\n",
            "** Example 101 **\n",
            "tom refused to pay.\n",
            "tom se negó a pagar .\n",
            "\n",
            "** Example 102 **\n",
            "stop fighting!\n",
            "¡ deja de compádo !\n",
            "\n",
            "** Example 103 **\n",
            "where's the red cross?\n",
            "¿ dónde está el rojo del rojo ?\n",
            "\n",
            "** Example 104 **\n",
            "let's just pray this never happens again.\n",
            "simplementemonos rezamos por nunca más .\n",
            "\n",
            "** Example 105 **\n",
            "we have put the christmas presents under the tree.\n",
            "hemos puesto nuestros regalos de navidad debajo del árbol .\n",
            "\n",
            "** Example 106 **\n",
            "is that why you want to come over?\n",
            "¿ es por eso lo que quieres venir ?\n",
            "\n",
            "** Example 107 **\n",
            "she can't drive a car.\n",
            "ella no sabe conducir un auto .\n",
            "\n",
            "** Example 108 **\n",
            "he's already too far away to hear us.\n",
            "él ya es demasiado lejos para que escundrá .\n",
            "\n",
            "** Example 109 **\n",
            "keep away from the fire.\n",
            "mantenga al fuego .\n",
            "\n",
            "** Example 110 **\n",
            "they don't want us working together.\n",
            "no nos quieres trabajar juntos .\n",
            "\n",
            "** Example 111 **\n",
            "it's more than enough.\n",
            "es más que ya .\n",
            "\n",
            "** Example 112 **\n",
            "tom has one foot in the grave.\n",
            "tom tiene el pie en el aceptado .\n",
            "\n",
            "** Example 113 **\n",
            "where's everyone else?\n",
            "¿ dónde está todos los demás ?\n",
            "\n",
            "** Example 114 **\n",
            "class doesn't begin until eight-thirty.\n",
            "las clases no empiezan hasta las ocho y media .\n",
            "\n",
            "** Example 115 **\n",
            "tom has to look after mary.\n",
            "tom tiene que mirar mary .\n",
            "\n",
            "** Example 116 **\n",
            "he should have done that long ago.\n",
            "él debería haber hecho hace mucho tiempo .\n",
            "\n",
            "** Example 117 **\n",
            "i don't approve of his conduct.\n",
            "no apruebe su condununce .\n",
            "\n",
            "** Example 118 **\n",
            "you have to obey your parents.\n",
            "tienes que obedecer a tus padres .\n",
            "\n",
            "** Example 119 **\n",
            "the book is white.\n",
            "el libro es blanco .\n",
            "\n",
            "** Example 120 **\n",
            "i crossed the dark street.\n",
            "yo crununtinceo a la calle .\n",
            "\n",
            "** Example 121 **\n",
            "i am married and i have a daughter.\n",
            "estoy casada y tengo una hija .\n",
            "\n",
            "** Example 122 **\n",
            "please come on monday.\n",
            "por favor vení el lunes .\n",
            "\n",
            "** Example 123 **\n",
            "he bought a pair of shoes.\n",
            "él se compró un par de zapatos .\n",
            "\n",
            "** Example 124 **\n",
            "i'm not your son.\n",
            "no soy tu hijo .\n",
            "\n",
            "** Example 125 **\n",
            "i'll stay home.\n",
            "me quedaré en casa .\n",
            "\n",
            "** Example 126 **\n",
            "if you want to speak to me, please call me up.\n",
            "si quieres hablar conmigo , por favor por favor .\n",
            "\n",
            "** Example 127 **\n",
            "tom tried to get mary to accept our proposal.\n",
            "tom trató de conseguir a mary que nuestra propuesta .\n",
            "\n",
            "** Example 128 **\n",
            "tom helped mary carry her suitcases.\n",
            "tom ayudó a mary a llevarle las manos la manez .\n",
            "\n",
            "** Example 129 **\n",
            "i have read that story in some book.\n",
            "he leído esa historia en algún libro .\n",
            "\n",
            "** Example 130 **\n",
            "i need to renew my passport.\n",
            "necesito remimune a mi pasaporte .\n",
            "\n",
            "** Example 131 **\n",
            "i fixed the bike yesterday.\n",
            "arreglé la bicicleta ayer .\n",
            "\n",
            "** Example 132 **\n",
            "you must return the book to him.\n",
            "debes regresar el libro a él .\n",
            "\n",
            "** Example 133 **\n",
            "don't try this at home.\n",
            "no intentes en casa .\n",
            "\n",
            "** Example 134 **\n",
            "the boy caught a large fish.\n",
            "el niño pilló un gran peces .\n",
            "\n",
            "** Example 135 **\n",
            "what exactly do you want?\n",
            "¿ qué quieres exactamente ?\n",
            "\n",
            "** Example 136 **\n",
            "with your talent, you should be able to make a lot of money.\n",
            "con talento tu talento disticuten a hacer mucho dinero .\n",
            "\n",
            "** Example 137 **\n",
            "don't forget who you are.\n",
            "no te olvides de quién eres .\n",
            "\n",
            "** Example 138 **\n",
            "there's a lot of traffic.\n",
            "hay mucho tráfico .\n",
            "\n",
            "** Example 139 **\n",
            "open your eyes, please.\n",
            "¡ abiertan ojos , por favor !\n",
            "\n",
            "** Example 140 **\n",
            "i asked tom to sing.\n",
            "le pedí a tom que cantara .\n",
            "\n",
            "** Example 141 **\n",
            "if you go to the movies, take your sister with you.\n",
            "si vas al cine , te toma tu hermana .\n",
            "\n",
            "** Example 142 **\n",
            "it was a hot day.\n",
            "era un día a día .\n",
            "\n",
            "** Example 143 **\n",
            "are we allowed to use the elevator?\n",
            "¿ le hemos pimimimo al ascensor ?\n",
            "\n",
            "** Example 144 **\n",
            "i'm not insane.\n",
            "no soy de la momincié .\n",
            "\n",
            "** Example 145 **\n",
            "i should've never come here.\n",
            "debería haber venido nunca aquí .\n",
            "\n",
            "** Example 146 **\n",
            "japan is not what it was ten years ago.\n",
            "japón no es lo que era hace diez años atrás .\n",
            "\n",
            "** Example 147 **\n",
            "where does that idiot think he is going?\n",
            "¿ dónde va a pensar que va a pensar ?\n",
            "\n",
            "** Example 148 **\n",
            "we could make it on time if we walked a little faster.\n",
            "podría hacerlo cuando entreguamos un poco más rápido .\n",
            "\n",
            "** Example 149 **\n",
            "where do the airport buses leave from?\n",
            "¿ dónde viene el aeropuerto ?\n",
            "\n",
            "** Example 150 **\n",
            "watch carefully.\n",
            "mira bien .\n",
            "\n",
            "** Example 151 **\n",
            "this is the temple which we used to visit.\n",
            "esta es el templela que solía visitar .\n",
            "\n",
            "** Example 152 **\n",
            "the teacher began to ask us questions.\n",
            "el profesor nos comenzó a hacer preguntas .\n",
            "\n",
            "** Example 153 **\n",
            "how many different schools have you attended?\n",
            "¿ cuántos modeciones a usted ?\n",
            "\n",
            "** Example 154 **\n",
            "i've seen him before.\n",
            "lo he visto antes .\n",
            "\n",
            "** Example 155 **\n",
            "what didn't i write?\n",
            "¿ qué no escribe eso ?\n",
            "\n",
            "** Example 156 **\n",
            "i am no better than a beggar.\n",
            "no soy mejor que no sea agañar a mí .\n",
            "\n",
            "** Example 157 **\n",
            "tom asked his father if he could go to the movies.\n",
            "tom le preguntó a su padre si podía ir al cine .\n",
            "\n",
            "** Example 158 **\n",
            "where's the other one?\n",
            "¿ dónde está el otro ?\n",
            "\n",
            "** Example 159 **\n",
            "we watched tv after lunch.\n",
            "vimos la televisión después del almuerzo .\n",
            "\n",
            "** Example 160 **\n",
            "you want me to show you how to do it?\n",
            "¿ quieres que te en qué hacer lo hagas ?\n",
            "\n",
            "** Example 161 **\n",
            "could you just please answer the question? we don't have all day.\n",
            "¿ podrías solo responder a la pregunta , por favor ? no tenemos el día .\n",
            "\n",
            "** Example 162 **\n",
            "the water came up to my knees.\n",
            "el agua se me combrantiló mis rodillas .\n",
            "\n",
            "** Example 163 **\n",
            "spain ruled cuba at that time.\n",
            "españa era el palleto en ese momento .\n",
            "\n",
            "** Example 164 **\n",
            "tom wanted mary to stay a little bit longer.\n",
            "tom quería que mary se quedara un poco más tiempo .\n",
            "\n",
            "** Example 165 **\n",
            "don't put the wallet on the top of the heater.\n",
            "no te las aplatas de la recordara en la cima .\n",
            "\n",
            "** Example 166 **\n",
            "i took a day off last week.\n",
            "me tomó un día libre la semana pasada .\n",
            "\n",
            "** Example 167 **\n",
            "tom is a freelance photographer.\n",
            "tom es una foto asustaraturrió .\n",
            "\n",
            "** Example 168 **\n",
            "i am no better than a beggar.\n",
            "no soy mejor que no sea agañar a mí .\n",
            "\n",
            "** Example 169 **\n",
            "she made thirty thousand dollars.\n",
            "ella hizo 000 dólares con treinta dólares .\n",
            "\n",
            "** Example 170 **\n",
            "it went well.\n",
            "se fue bien .\n",
            "\n",
            "** Example 171 **\n",
            "you need to help me.\n",
            "necesitas ayudarlo .\n",
            "\n",
            "** Example 172 **\n",
            "i second the motion.\n",
            "me mordético .\n",
            "\n",
            "** Example 173 **\n",
            "steel traps are illegal.\n",
            "las trampas para la trampas son ilegales .\n",
            "\n",
            "** Example 174 **\n",
            "tom listened with interest.\n",
            "tom escuchó con interés .\n",
            "\n",
            "** Example 175 **\n",
            "my father often helps me with my homework.\n",
            "mi padre a menudo ayuda a mi tarea .\n",
            "\n",
            "** Example 176 **\n",
            "i am at home.\n",
            "estoy en casa .\n",
            "\n",
            "** Example 177 **\n",
            "you are banned from entering this place.\n",
            "eres intendunvers del lugar de entrar en este lugar .\n",
            "\n",
            "** Example 178 **\n",
            "cranes are big beautiful birds.\n",
            "los creradmpús son grandes pájaros hermosas .\n",
            "\n",
            "** Example 179 **\n",
            "that's hardly believable.\n",
            "eso es casi malo .\n",
            "\n",
            "** Example 180 **\n",
            "he frequently neglects his work.\n",
            "crechis incupen unos nuctictisidad chis precista .\n",
            "\n",
            "** Example 181 **\n",
            "tom ought to stop smoking.\n",
            "tom debería dejar de fumar .\n",
            "\n",
            "** Example 182 **\n",
            "tom is just a little bit shorter than you.\n",
            "tom es sólo un poco más corto de lo que tú .\n",
            "\n",
            "** Example 183 **\n",
            "i'd like a cup of tea.\n",
            "me gustaría una taza de té .\n",
            "\n",
            "** Example 184 **\n",
            "we were cooking tempura at that time.\n",
            "estábamos realmente compuriar el momento en ese momento .\n",
            "\n",
            "** Example 185 **\n",
            "the thief ran away and the policeman ran after him.\n",
            "el ladrón entró corriendo y el policía le corrió después .\n",
            "\n",
            "** Example 186 **\n",
            "i became very nervous when i couldn't locate my passport.\n",
            "me he a la atento muy nervioso cuando yo no podía mi pasaporte .\n",
            "\n",
            "** Example 187 **\n",
            "you run.\n",
            "te copento .\n",
            "\n",
            "** Example 188 **\n",
            "she watched him draw a picture.\n",
            "ella lo vio dibujante .\n",
            "\n",
            "** Example 189 **\n",
            "they faced the danger bravely.\n",
            "ellos en la cara de aparecerable .\n",
            "\n",
            "** Example 190 **\n",
            "she takes part in many school activities.\n",
            "ella tiene precticticti a muchas personas mostutividad .\n",
            "\n",
            "** Example 191 **\n",
            "he came running.\n",
            "él vino corriendo .\n",
            "\n",
            "** Example 192 **\n",
            "from now on, i promise to be punctual.\n",
            "de ahora en promesa , pulenfa .\n",
            "\n",
            "** Example 193 **\n",
            "tom was helping me.\n",
            "tom me estaba ayudar .\n",
            "\n",
            "** Example 194 **\n",
            "she was a pioneer in this field.\n",
            "ella fue un piada en este campo .\n",
            "\n",
            "** Example 195 **\n",
            "tom finally came up with the cash.\n",
            "tom finalmente se le cayó con el cabece .\n",
            "\n",
            "** Example 196 **\n",
            "it's okay.\n",
            "está bien .\n",
            "\n",
            "** Example 197 **\n",
            "the king once lived in that palace.\n",
            "el rey vivió una vez en ese paltiva .\n",
            "\n",
            "** Example 198 **\n",
            "i wonder if this is love.\n",
            "me pregunto si esto es amor .\n",
            "\n",
            "** Example 199 **\n",
            "he hung up on me.\n",
            "él me se derro encen .\n",
            "\n"
          ]
        }
      ]
    }
  ]
}